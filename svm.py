import os
import cv2 as cv
import joblib
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from torch import cdist, tensor

from utils import get_image_paths, read_img

DATA_PATH = 'data'
IMAGE_CATEGORIES = [
    'black widow', 'captain america', 'doctor strange', 'hulk', 'ironman', 'loki', 'spider-man', 'thanos'
]
SIFT_MAX_FEATURES = 50
CODEBOOK_FILE = 'codebook.joblib'
SVM_MODEL_FILE = 'svm_bow_mode.joblib'


'''
    Build Codebook, 
    1) Taking a set of train images and describing with SIFT
    2) Using K-means clustering to cluster the descriptors 
    
    returning cluster centers
'''
def build_codebook(image_paths, num_tokens = 15):
    sift = cv.SIFT_create() 
    container = []
    
    for image_path in image_paths:
        img = read_img(image_path)
        keypoints, descriptors = sift.detectAndCompute(img, None)
        if descriptors is not None:
            container.append(descriptors)
    
    container = np.concatenate(container)

    #Training KMeans
    kmeans = KMeans(n_clusters = num_tokens)

    #Fit data with descriptors
    kmeans.fit(container)
    print('Fitted KMeans with descriptors')
   
    return kmeans.cluster_centers_


'''
Returns the image features as a list of bag of words generated for each image
@param image_paths is a list of all image paths to be turned into a bag of words
@param codebook previously generated by 
    1) Taking a set of train images and describing with SIFT
    2) Using K-means clustering to cluster the descriptors
Bag of words takes the descriptors from training images and matches them to the nearest cluster center
We then note the number points matched to each cluster center, forming a "histogram" of features
We do this for all the images, returning an image_features array containing a list of bag-of-words for each image
'''
def bag_of_words(image_paths, codebook, images_array = None):
    sift = cv.SIFT.create(nfeatures = SIFT_MAX_FEATURES)
    codebook_size = codebook.shape[0]

    print(codebook_size)

    image_features = []
    cb = tensor(codebook)

    if not images_array and image_paths:
        for image_path in image_paths:
            img = read_img(image_path)
            keypoints, descriptors = sift.detectAndCompute(img, None)
            #Initialise bow
            bow = np.zeros(codebook_size)
            if descriptors is not None:
                #Calculates distances btw descriptors and cluster centers
                des = tensor(descriptors)
                distances = cdist(des, cb)
                for d in distances:
                    #Get the index of min dist for each keypt-desc to center
                    bow[np.argmin(d)] += 1
            image_features.append(bow.reshape(1, codebook_size))
    
    elif images_array and not image_paths:
        for image in images_array:
            img = image
            keypoints, descriptors = sift.detectAndCompute(img, None)
            #Initialise bow
            bow = np.zeros(codebook_size)
            if descriptors is not None:
                #Calculates distances btw descriptors and cluster centers
                des = tensor(descriptors)
                distances = cdist(des, cb)
                for d in distances:
                    #Get the index of min dist for each keypt-desc to center
                    bow[np.argmin(d)] += 1
            image_features.append(bow.reshape(1, codebook_size))
    else:
        print("No images provided")
        

    
    image_features = np.concatenate(image_features)
    return image_features



if __name__ == '__main__':
    #Get image paths
    train_image_paths, test_image_paths, train_labels, test_labels =\
        get_image_paths(DATA_PATH, IMAGE_CATEGORIES, 600, 30, 'valid')
    
    #Load or build codebook
    if os.path.exists(CODEBOOK_FILE):
        print("Loading Codebook")
        codebook = joblib.load(CODEBOOK_FILE)
    else:
        print('Building Codebook...')
        codebook = build_codebook(train_image_paths)
        
        joblib.dump(codebook, CODEBOOK_FILE)
        print('Built Codebook.')
    
    scaler = StandardScaler()

    print('Generating BoW features for training set...')
    train_images = bag_of_words(train_image_paths, codebook)
    train_images_scaled = scaler.fit_transform(train_images)
    print('Train images:', train_images.shape)

    print('Generating BoW features for test set...')
    test_images = bag_of_words(test_image_paths, codebook)
    test_images_scaled = scaler.fit_transform(test_images)
    print('Test images:', test_images.shape)



    if os.path.exists(SVM_MODEL_FILE):
        print('Loading existing linear SVM model...')
        svm = joblib.load(SVM_MODEL_FILE)
    else:
        print('Training a linear SVM...')
        svm = SVC(gamma = 'scale')
        svm.fit(train_images_scaled, train_labels)
        joblib.dump(svm, SVM_MODEL_FILE)
        print('Trained SVM Model')
    
    test_predictions = svm.predict(test_images_scaled)
    accuracy = accuracy_score(test_labels, test_predictions)
    print('Classification accuracy of SVM with BOW features: ', accuracy)

    #F1 score F1 = 2 * (precision * recall) / (precision + recall)
    #F1 score can be interpreted as a harmonic mean of the precision and recall, 
    #where an F1 score reaches its best value at 1 and worst score at 0
    f1 = f1_score(test_labels, test_predictions, average='macro')
    print('Classification f1 score(macro averaged) of SVM with BOW features: ', f1)




